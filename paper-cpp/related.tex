
%\input{discussion.tex}

The Coq standard library has linear-time definitions
of division and base-$2$ discrete logarithm on \li{nat} and \li{N}.
The Mathematical Components library~\cite{MathComp}
has a discrete logarithm with arbitrary base, with inputs encoded in \li{nat}.
The Isabelle/HOL Archive of Formal Proofs~\cite{isastan2019} 
provides a definition of discrete logarithm
with arbitrary base along with a separTate computation strategy (\lstset{style=isaStyle}\li{[code]}\lstset{style=myStyle}).
None of these libraries provides a definition of iterated logarithm or
further members of the hierarchy.
Indeed, to our knowledge, we are the first to generalize this
problem in a proof assistant, extend it both
upwards and downwards in the natural hierarchy of functions, and
provide linear-time computations.

\subsection{Historical notes}
The operations of successor, predecessor, addition, and subtraction have
been integral to counting forever. The ancient Egyptian
number system used glyphs denoting $1$, $10$, $100$, \emph{etc.},
and expressed numbers using additive combinations of these.
The Roman system is similar, but
combines glyphs using both addition and subtraction.
This buys brevity and readability,
since \emph{e.g.} $9_{\text{roman}}$ is two characters, ``one less than ten'',
and not a series of nine $1$s.
The ancient Babylonian system was the first to introduce 
\emph{algorisms}: the place value of a glyph succinctly expressed \emph{how many times} 
it counted towards the number being represented, along with associated techniques for addition, multiplication, \emph{etc.}
The Babylonians operated in
base $60$, and so \emph{e.g.} a three-gylph number $abc_{\text{babylonian}}$ could
be parsed as $a \times 60^2 + b \times 60 + c$. Sadly they lacked
a radix point, and so
$a \times 60^3 + b \times 60^2 + c \times 60$, $a \times 60 + b + c \div 60$,
\emph{etc.} were also reasonable interpretations, and the correct number had
to be inferred from context.
Using multiplication and division in numerical representations bought great concision: a number $n$ was
represented in $\lfloor \log_{60}n \rfloor + 1$ glyphs.
The modern Indo-Arabic decimal system is also an algorism, 
but operates in base $10$ and (thankfully) has a radix point.

A form of exponentiation was discussed by Euclid (\textasciitilde 300 BCE), and our modern
understanding of it, including the superscript notation,
came in 1637 thanks to René Descartes~\cite{descartes}. 
Similarly, logarithms were mentioned by 
Archimedes (\textasciitilde 250 BCE) and our modern understanding came in 1614 thanks to 
John Napier~\cite{napier}.

The three-variable Ackermann function was presented by Wilhelm Ackermann~\cite{ackermann} in 1928 as an example of a total computable function that is not primitive recursive.
In 1935, Rózsa Péter~\cite{peter} developed a two-argument variant of the Ackermann 
function, and it is her variant, often called the Ackermann-Péter function,
that computer scientists---the authors included---commonly care about.
In 1947, Reuben Goodstein~\cite{goodstein} showed that a variant of the Ackermann function 
can be used to place the natural sequence of functions (addition, multiplication,
exponentiation) in a systematic hierarchy of hyperoperations. 
This brought tetration, the fourth member of the sequence, into use.
In the 1980s, computer scientists started using the
iterated logarithm $\log^*$ in algorithmic analysis.

\subsection{Inverse Ackermann in computer science}
% Ackermann's original function does not have the higher-order
% relation to repeated application and hyperoperation that we have been studying in
% this paper. Those properties emerged thanks to refinements by ,

The inverse of the Ackermann function
features in the time bound analyses of several algorithms.
Tarjan~\cite{tarjan} showed that the union-find data structure
takes time \mbox{$O(m\cdot\alpha(m,n))$} for a sequence of $m$ operations
involving no more than $n$ elements.
Tarjan and van Leeuwen~\cite{tarjan2} later refined this to $O(m\cdot\alpha(n))$.
Chazelle~\cite{chazelle} showed that the minimum spanning tree
of a connected graph with $n$ vertices and $m$ edges
can be found in time $O(m\cdot\alpha(m,n))$.

Chargu\'eraud and Pottier~\cite{charpott}
verify the time complexity of union-find in Coq.
% Their analysis requires inverse Ackermann, and 
To this end they invert another variant of the Ackermann function 
(\emph{i.e.} not the Ackermann-Péter function we have 
studied). 
However, their inversion strategy 
relies on Hilbert's non-constructive~$\varepsilon$ operator 
to choose the necessary minimum (see Definition~\ref{defn: inv_ack}). 
% The~$\varepsilon$ operator blurs the distinction between \li{Prop} and \li{Type}, 
This collapses Coq's distinction between \li{Prop} and \li{Type}, meaning that
Chargu\'eraud and Pottier's 
inverse cannot be extracted to executable code.  Moreover, 
Coq tactics such as \li{compute} cannot reduce applications of
their inverse to concrete values.  In contrast, with our technique,
proving a goal such as \li{inv_ack_linear 100 = 4} is as simple as 
using \li{reflexivity}, and the computation is essentially instantaneous.

%Every pearl starts with a grain of sand.  We had the benefit of two:
%Nivasch~\cite{nivasch} and Seidel~\cite{seidel}.
%They proposed a definition of the inverse Ackermann essentially in terms of
%the inverse hyperoperations, \emph{i.e.} along the lines of our Theorem~\ref{thm: inv-ack-new}: $\forall n.~ \alpha(n) \stackrel{?}{=} \Theta(\min\big\{k : 2 \langle k \rangle n\le 3 \big\})$.  Unfortunately, their technique is unsound (it should be $\le k$), since it diverges from
%the true Ackermann inverse when the inputs grow sufficiently large; even adjusting for this, their technique is off by an additive constant.  Our technique is exact and verified in Coq.

\section{Conclusion}
We have implemented a hierarchy of functions that calculate the upper inverses
to the hyperoperation/Ackermann hierarchy and used these inverses
to compute the inverse of the diagonal Ackermann function~$\Ack(n)$.
Our functions run in~$\Theta(b)$ time on unary-represented inputs.  On
binary-represented inputs, our \mbox{base-2} hyperoperations and inverse Ackermann
run in~$\Theta(b)$ time as well, where~$b$ is bitlength; our general binary 
hyperoperations run in~$O(b^2)$.
Our functions are structurally recursive,
and thus easily satisfy Coq's termination checker.

Every pearl starts with a grain of sand.  We had the benefit of two:
Nivasch~\cite{nivasch} and Seidel~\cite{seidel}.
They proposed a definition of the inverse Ackermann essentially in terms of
the inverse hyperoperations.  However, their definition is approximate rather than exact (\emph{i.e.} it has the same asymptotic rate of growth as the canonical definition but is off by a bounded-but-varying constant factor) and they provide only a sketch of an algorithm.  Our technique is exact; our algorithm concrete, structurally recursive and asymptotically optimal; and our theory is verified in Coq.

%, and that it is consistent with
%the usual definition of the inverse Ackermann function $\alpha(n)$.


%{\color{magenta}This is a direction
%we intend to explore to formally check the $O(n)$ bound
%of the inverse Ackermann function.}

%Cite: Ackermann, Peter, Tarjan, Chazelle, Pottier? Anything in HOL? Anything in SSReflect?

%\paragraph*{Alternative strategies}


% Other ways to skin the cat.
% - You can define division via mutual recursion (subtraction and division simultaenously).
% - The inverse ackerman-lite by Anshuman.
% - The automata technique.
% - Binary representations
% - Division by constant, etc. is simpler.
% - Custom termination metrics.  Gas.
% - Space, tail recursion, time?

%\paragraph*{Other?} 