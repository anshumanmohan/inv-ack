\newcommand{\ackt}{\ensuremath{\hat{\alpha}}}

\subsection{A performance improvement with binary numbers}
\hide{
Thus far we have used the Coq type \li{nat}, which is a unary system of 
natural numbers no more sophisticated than tally marks: 
a number~$n$ is represented in~$n$ bits. 
The binary system represents numbers using $0$s and $1$s, thus 
allowing~$n$ to be represented in $\lfloor \log_{2} n \rfloor + 1$ bits.
In both systems, addition and subtraction of two $n$-bit numbers take $\Theta(n)$ time, 
while multiplication takes $\Theta \big(n^2\big)$ time. 
The binary system thus easily outperforms the unary in basic arithmetic operations. 
}
Thus far we have used the Coq type \li{nat}, which is a unary system of 
natural numbers no more sophisticated than tally marks: 
a number~$n$ is represented in~$n$ bits. 
The binary system represents~$n$ in $\lfloor \log_{2} n \rfloor + 1$ bits.
In both systems, addition and subtraction of two $b$-bit 
numbers take $\Theta(b)$ time, 
while multiplication takes $\Theta \big(b^2\big)$ time. 
Basic arithmetic operations are therefore much faster when the inputs 
are encoded in binary. 
Here we show that this advantage carries forward to the computation of 
hyperoperations, Ackermann functions, and their inverses. 
We put a further explanation of the Coq-supported binary type \li{N} in 
Appendix~\ref{apx:bin_in_coq}.
\hide{
Here we show that this advantage carries forward to computing inverses of functions in the hyperoperations and Ackermann hierarchies, as well as the inverse Ackermann function itself.
Coq has support for binary numbers with the type \li{N}, which consists 
of constructors \li{0} and \li{positive}:
\begin{lstlisting}
Inductive positive : Set := 
  | xI : positive -> positive | xO : positive -> positive
  | xH : positive.
\end{lstlisting}
Constructor \li{xH} represents $1$, and constructors \li{xO} and \li{xI} represent 
appending $0$ and $1$ respectively. 
By always starting with $1$, \li{positive} dodges
the issue of disambiguating \emph{e.g.} the numbers \li{011} and 
\li{00011}, which represent the same number but pose
a minor technical challenge. 
To represent $0$, the type \li{N} simply provides a separate constructor \li{0}. 
%\paragraph*{Exponentiation with Binary Numbers}
%This a massive improvement over the bound $\Theta\left(a^b\right)$ of exponentiation in the unary system.
}
\hide{
\begin{lstlisting}
Definition exp (x y : N) : N := match y with
| 0 => 1 | Npos y'
  => match x with
     | 0 => 0 | _ => 
     let fix expPos (p : positive) :=
            match p with
            | xH => x
            | xI p' => let t := expPos p' in x * t * t
            | xO p' => let t := expPos p' in t * t
            end in expPos y'
     end end.
\end{lstlisting}
To analyse the performance of this definition, let us denote the computation time for \li{exp a b} and \li{a * b} respectively by $\Texp (a, b)$ and $ \Tmul (a, b)$.Now
\begin{equation*}
\begin{aligned}
\Texp(a, b)
& \le \Texp\left(a, \left\lfloor \frac{b}{2} \right\rfloor\right)
+ \Tmul\left(a^{\left\lfloor \frac{b}{2} \right\rfloor}, a^{\left\lfloor \frac{b}{2} \right\rfloor} \right) + \Tmul\left(a^{2\left\lfloor \frac{b}{2} \right\rfloor}, a\right) \\
& \le \Texp\left(a, \left\lfloor \frac{b}{2} \right\rfloor\right) + \left\lfloor \frac{b}{2} \right\rfloor^2 \big(\log_2 a + 1\big)^2 + 2\left\lfloor \frac{b}{2} \right\rfloor \big(\log_2 a + 1\big)^2 \\
& \le \Texp\left(a, \left\lfloor \frac{b}{2} \right\rfloor\right) + 3\left(\frac{b}{2}\right)^2 \big(\log_2 a + 1\big)^2
\end{aligned}
\end{equation*}
Thus, $\Texp(a, b) \le 3\big(\log_2 a + 1\big)^2 \sum_{k=1}^{\infty} \frac{b^{2k}}{4^k} = \bigO\left(b^2\big(\log_2 a + 1\big)^2\right)$. 
This a massive improvement over the bound $\Theta\left(a^b\right)$ of exponentiation in the unary system.
} % end hide

\paragraph*{Countdown and Contractions with Binary Numbers.}
\hide{Apart from speeding up addition and multiplication, the binary system does the same for their successor, namely exponentiation. We achieve this by \emph{repeated squaring},
which by standard techniques allows us to calculate $a^b$ in $\bigO\left(b^2\big(\log_2 a + 1\big)^2\right)$.} % end hide

Although the theoretical \emph{countdown} is independent of the encoding 
of its inputs, its Coq definition needs to be adjusted to allow for inputs 
in \li{N}. The first step is to translate the arguments of 
\emph{countdown worker} from \li{nat} to \li{N}. While the type of 
the budget \li{k} should remain \li{nat} as it must decrease by $1$ on 
every recursive step, all other \li{nat} arguments should be changed 
to \li{N}, and functions on \li{nat} to functions on \li{N}.
\begin{lstlisting}
Fixpoint bin_countdown_worker (f : N -> N) (a n : N) (b : nat) : N :=
  match b with
  | O    => 0
  | S b' => if (n <=? a) then 0
             else 1 + bin_countdown_worker f a (f n) b'
end.
\end{lstlisting}
Determining the budget for \li{bin\_countdown} is trickier. 
The naÃ¯ve approach is to simply use the \li{nat} translation of \li{n}, 
\emph{i.e.} \li{N.to_nat n}. However, this is untenable as the conversion 
to \li{nat} alone takes exponential time 
(\emph{viz} the length of its representation).
Unfortunately, a linear-sized budget is necessary to compute countdowns 
of functions like $\lambda n.(n-2)$, which is used by both the inverse 
hyperoperation and inverse Ackermann hierarchies.

The key is to ignore these functions entirely and focus on functions that can bring their argument to below any threshold with repeated applications in logarithmic time, which allows a logarithm-sized budget for countdown worker. The following concept aims to describe a type of \emph{fast-contracting} functions.
\begin{defn} \label{defn: bin-contraction}
	$f\in \contract$ is \emph{binary strict above} $a\in \mathbb{N}$ if $\forall n > a, f(n) \le \lfloor \frac{n + a}{2} \rfloor$.
\end{defn}
The key advantage of binary strict contractions is that if a contraction $f$ is binary strict above some $a$, then for all $n > a$ and $k$,
$f(n) \le \left\lfloor \frac{n - a}{2^k} \right\rfloor + a$.
Therefore within $\lfloor \log_2 (n - a) \rfloor + 1$ compositional applications of $f$, the result will become equal to or less than $a$, so we can choose this number as the budget for \emph{countdown worker} to successfully reach the correct countdown value before terminating. Our last step is to compute this budget as a \li{nat} given the number $n$ of type \li{N}, \note[consider moving this Coq definition to appendix and give a one-liner explanation here]{which is done with the following function:}
\begin{lstlisting}
Definition nat_size (n : N) : nat :=
  match n with
  | 0 => 0%nat
  | Npos p => let fix nat_pos_size (x : positive) : nat :=
                  match x with
                  | xH => 1%nat
                  | xI y | xO y => S (nat_pos_size y) 
                  end
              in nat_pos_size p
  end.
\end{lstlisting}
Note that \li{nat_size} outputs $0$ on $0$, and on any positive number $m$ the size of its binary representation, thus equals to $\lfloor \log_{2} m \rfloor + 1$.
To sum up, we have the following Coq definition of countdown on \li{N}.
\begin{lstlisting}
Definition bin_countdown (f : N -> N) (a n : N) : N := 
  bin_countdown_worker f a n (nat_size (n - a)).
\end{lstlisting}

\subsubsection*{Time complexity of countdown on strict binary contractions}
We start by inspecting the execution time of \li{leb} and \li{succ} for the binary encoding system. While \li{leb} is trivial, \li{succ} requires more care as it is worst-case logarithmic time but amortized constant time.
\begin{lem} \label{lem: leb-runtime-bin}
	In the binary encoding system, $\runtime_{\li{leb}}(n, a) = \lfloor \log_{2}\min\{a, n\} \rfloor + 1$.
\end{lem}
\begin{lem} \label{lem: succ-runtime-bin}
	For all $n\ge 1$, $S(n) = \displaystyle \sum_{i=0}^{n-1}\Tsucc(i) \le 2n + \log_2n$.
\end{lem}
\begin{proof}
	In the binary encoding system, $\Tsucc(u)$ is the number of consecutive least significant \li{1} bits of $u$.
	If $n = 1$, $\Tsucc(0) = 1$, our goal is $1\le 2$, which is trivial. For $n > 1$, observe that for all $k$, $\Tsucc(k) = \Tsucc\left(\frac{k-1}{2}\right) + 1$ if $k$ is odd and $1$ if $k$ is even. Thus for all $n$, $S(2n+1) = S(2n) + 1$ and
	\begin{equation*}
	\begin{aligned}
	S(2n) & = \ \sum_{i=0}^{n}\Tsucc(2i) + \sum_{i=0}^{n-1}\Tsucc(2i+1)
	= \ \sum_{i=0}^{n}1 + \sum_{i=0}^{n-1}\big(\Tsucc(i) + 1\big) \\
	& = n+1 + S(n) + n = S(n) + 2n + 1
	\end{aligned}
	\end{equation*}
	From the above, it is easy to complete the proof via induction on $n$.
\end{proof}
The following lemma is the version of Lemma~\ref{lem: cdt-runtime} for the binary system.
\begin{lem} \label{lem: cdt-runtime-bin}
	In the binary encoding system, if $f$ is a binary strict contraction above $a$, we have
	\begin{equation*}
	\runtime_{\cdt{f}{a}}(n) \le \sum_{i=0}^{\cdt{f}{a}(n) - 1} \hspace{-6pt}
	\runtime_f\big(f^{(i)}(n)\big) \ + \ (\log_2a + 3)\left(\cdt{f}{a}(n) + 1\right) \ + \ 2\log_2n \ + \ \log_2\cdt{f}{a}(n)
	\end{equation*}
\end{lem}
\begin{proof}
	On strict binary contractions, the binary version of countdown does exactly the same computations as the original one on \li{nat}, with the added step of computing the budget of \li{nat_size (n-a)}, thus its execution time structure is almost the same. The only difference is the running time of each component in the sum.
	\begin{equation} \label{eq: cdt-bin-runtime-struct}
	\runtime_{\cdt{f}{a}}(n) =
	\hspace{5pt}
	\begin{aligned} 
	& \textstyle \sum_{i=0}^{\cdt{f}{a}(n) - 1} \runtime_f\left(f^{(i)}(n)\right)
	&\hspace{-4pt}+& \textstyle ~ \sum_{i=0}^{\cdt{f}{a}(n)}\Tleb\left(f^{(i)}(n), a\right) \\
	& \textstyle +~ \sum_{i=0}^{\cdt{f}{a}(n) - 1}\Tsucc(i)
	&\hspace{-4pt}+& ~ \runtime_{\li{sub}}(n, a) \ + \ \runtime_{\li{nat_size}}(n - a)
	\end{aligned},
	\end{equation}
	By Lemma~\ref{lem: leb-runtime-bin}, $\forall i, \Tleb\left(f^{(i)}(n), a\right) \le \log_2a + 1$, thus the second summand in \eqref{eq: cdt-bin-runtime-struct} is no more than $(\log_2a + 1)\big(\cdt{f}{a}(n) + 1\big)$. By Lemma~\ref{lem: succ-runtime-bin}, the third summand is no more than $2\cdt{f}{a}(n) + \log_2\cdt{f}{a}(n)$. Lastly, the last two summands are each no more than $\log_2n + 1$. Thus
	 \begin{equation*}
	 \begin{aligned}
	 & \runtime_{\cdt{f}{a}(n)}(n) \ \le \
	 \left(\begin{aligned}
	 & \textstyle \sum_{i=0}^{\cdt{f}{a}(n) - 1} 
	 \runtime_f\big(f^{(i)}(n)\big)
	 \ + \ (\log_2a + 1)\big(f^{(i)}(n) + 1\big) \\
	 & \ + \ 2\cdt{f}{a}(n) \ + \ \log_2\cdt{f}{a}(n)
	 \ + \ 2\big(\!\log_2n + 1\big)
	 \end{aligned}\right)\\
	 & \le \ \textstyle \sum_{i=0}^{\cdt{f}{a}(n) - 1}
	 \runtime_f\big(f^{(i)}(n)\big) \ + \ (\log_2a + 3)\left(\cdt{f}{a}(n) + 1\right) \ + \ 2\log_2n \ + \ \log_2\cdt{f}{a}(n)
	 \end{aligned}
	 \end{equation*}
\end{proof}
%Substituting $a=1$ into \eqref{eq: cdt-runtime-bin} shows that \Cref{lem: inv-ack-hier-runtime} still holds.
%Similar to \Cref{sect: hardcode-lvl2}, the use of binaries is not immediately effective since the first level. We delve deeper into the hierarchy by \emph{hardcoding the $3^{\text{th}}$ level} and starts from there. Now
%\begin{equation*}
%\forall n, n+2  < n+3 < 2(n+2) \iff \forall n,
%\lfloor \log_2(n+2) \rfloor < \lceil \log_2(n+3) \rceil \le \lfloor \log_2(n+2) \rfloor + 1
%\end{equation*}
%This shift from floor to ceiling enables a direct computation, since $\lceil \log_2(n+3) \rceil$ can now be seen as the size of $(n+2)$'s binary representation.
%\begin{lstlisting}
%Definition alpha3 (n: N) : N := N.size (n+2) - 3.
%\end{lstlisting}
%Let $n\ge 2$. The computation of \li{N.size(n)} takes time equal to itself, so the above definition gives $\runtime(\alpha_3, n) \le 2\log_2n$. Fix an $i\ge 3$ and suppose $\runtime(\alpha_i, n) \le C_i\log_2n$. By \Cref{lem: inv-ack-hier-runtime},
\subsubsection*{An inverse Ackermann computation in $\Theta\left(\log_2 n\right)$ time}
Our new Coq definition is able to compute the correct countdown value, but only for strict binary contractions. Fortunately, starting from $n = 2$, the inverse hyperoperations $a\angle{n}b$ when $a\ge 2$ and the inverse Ackermann hierarchy $\alpha_n$ are all strict binary contractions. We can construct these hierarchies by hardcoding their first four levels and use countdown to recursively build higher levels. We focus on the inverse Ackermann hierarchy in this paper, but the hyperoperations' definition can also be found in our codebase, along with their proofs of correctness. Furthermore, for the purpose of optimizing the computation time, we also hardcode the fourth level in the below Coq Fixpoint.
\begin{lstlisting}
Fixpoint bin_alpha (m : nat) (x : N) : N :=
  match m with
  | 0%nat => x - 1          | 1%nat => x - 2
  | 2%nat => N.div2 (x - 2) | 3%nat => N.log2 (x + 2) - 2
  | S m'  => countdown (bin_alpha m') 1 (bin_alpha m' x)
  end.
\end{lstlisting}
Note that for all $x$, $\li{N.div2}(x - 2) = \left\lfloor \frac{x - 2}{2} \right\rfloor = \left\lceil \frac{x - 3}{2} \right\rceil$ and $\li{N.log2}(x + 2) - 2 = \left\lfloor \log_2(x+2) \right\rfloor - 2 = \left\lceil \log_2(x+3) \right\rceil - 3$, so the above Coq code is correct. Next, we define the \emph{inverse Ackermann worker}.
\begin{lstlisting}
Fixpoint bin_inv_ack_worker (f : N -> N) (n k : N) (b : nat) : N :=
  match b with
  | 0%nat  => k
  | S b' => if n <=? k then k
              else let g := (bin_countdown f 1) in
                bin_inv_ack_worker (compose g f) (g n) (N.succ k) b'
  end.
\end{lstlisting}
Note that the above Coq Fixpoint is the direct translation of the \li{inv_ack_worker} on \li{nat}, with only the budget remaining at type \li{nat} as it is the decreasing argument. Next we discuss its time complexity.
\begin{thm}
	$\forall i, \forall n, \runtime_{\alpha_i}(n) \le 2\log_2n + \left(3\cdot 2^i - 3i - 13\right)\log_2\log_2n + 3i$.
\end{thm}
\begin{proof}
	Firstly, we have:
	\begin{itemize}
		\item Clearly $\runtime_{\alpha_i}(n)\le \log_2n + 1$ for $i = 0, 1$.
		\item Computing $\alpha_2(n)$ consists of 2 steps: subtracting by 2 and dividing by 2 (shifting right by 1 bit), both of which takes no more than time $\log_2n + 1$ time, thus $\runtime_{\alpha_2}(n)\le 2\log_2n$.
		\item Computing $\alpha_3(n)$ consists of 3 steps: adding 2, then take base 2 logarithm and subtract 2 from the result, with the first two taking no longer than time $\log_2n + 1$ each and the last taking time at most $\log_2\log_2(n+2) + 1$ $\le \log_2\log_2 n + 2$. Thus $\runtime_{\alpha_3}(n)\le 2\log_2n + \log_2\log_2n + 3$.
	\end{itemize}
  Let $M_i = 3\cdot 2^i - 3i - 13$, then $\runtime_{\alpha_i}(n)\le 2\log_2n + M_i\log_2^{(2)}n + 3i$ for $i = 0, 1, 2, 3$.
	Suppose the same inequality holds for $i\ge 3$, by Lemmas~\ref{lem: compose-runtime} and \ref{lem: cdt-runtime-bin}, note that Lemma~\ref{lem: compose-runtime} works for all encoding systems, we have
	\begin{equation*}
	\begin{aligned}
	& \runtime_{\alpha_{i+1}}(n) \le
	\sum_{k=0}^{\alpha_{i+1}(n)} \hspace{-5pt} \runtime_{\alpha_i}\big(\alpha_i^{(k)}(n)\big)
	\ + \ 3\hspace{-4pt}\underbrace{\alpha_{i+1}(n)}_{\le \log_2\log_2n}
	+ \ 2\underbrace{\log_2\alpha_i(n)}_{\le \log_2\log_2n}
	+ \ \underbrace{\log_2\alpha_{i+1}(n)}_{\le \log_2\log_2n} + \ 3 \\
	& \le 2\log_2n
	+ 2\underbrace{\hspace{-5pt}\sum_{k=1}^{\alpha_{i+1}(n)}\hspace{-5pt} \log_2\alpha_i^{(k)}(n)}_{\le 2\log_2\log_2n}
	+ M_i\underbrace{\hspace{-5pt}\sum_{k=0}^{\alpha_{i+1}(n)}\hspace{-5pt} \log_2^{(2)}\alpha_i^{(k)}(n)}_{\le 2\log_2\log_2n}
	+ 3i\big(\hspace{-5.5pt}\underbrace{\alpha_{i+1}(n)}_{\le \log_2\log_2n}\hspace{-4pt} + 1\big) + 6\log_2^{(2)}n + 3 \\
	& \le 2\log_2n + (2M_i + 3i + 10)\log_2^{(2)}n + 3(i + 1)
	= 2\log_2 + M_{i+1}\log_2^{(2)}n + 3(i + 1)
	\end{aligned}
	\end{equation*}
	where $\forall i\ge 3, \alpha_i(n)\le \log_2n$ and $\alpha_{i+1}(n)\le \log_2^{(2)}n$, while the $2\log_2^{(2)}n$ upper bounds for the sums come from Lemma~\ref{lem: sum-alpha-repeat}.
	By induction on $i$, proof is complete.
\end{proof}
This lemma implies $\runtime_{\alpha_i}(n) = \Theta\big(\log_2n + 2^i\log_2\log_2n \big)$.

Now, the inverse Ackermann function on \li{N} can be computed by the following Coq definition, which is a mirror of the version on \li{nat} from Section~\ref{sect: hardcode-lvl2} with an extra hardcoded level:
\begin{lstlisting}
Definition bin_inv_ack (n : N) : N :=
       if (n <=? 1) then 0
  else if (n <=? 3) then 1
  else if (n <=? 7) then 2
  else let f := (bin_alpha 3) in
       bin_inv_ack_worker f (f n) 3 (nat_size n).
\end{lstlisting}
Note that for all $n > 7$, $n < \Ack\big(\lfloor \log_2n \rfloor + 1\big)$ $= \Ack\big(\li{nat_size}(n)\big)$, thus a budget of $\li{nat_size}(n)$ is sufficient for the worker to reach the correct inverse Ackermann value before terminating. The full proof of correctness for \li{bin_inv_ack} can be found in our codebase.

The time complexity of \li{bin_inv_ack} can be expressed as a sum of components similar to of~(\ref{eq: inv-ack-runtime-improved}).
\begin{equation*}
\runtime_\alpha(n) \ =
\begin{aligned}
& \sum_{k = 3}^{\alpha(n) - 1} \runtime_{\cdt{(\alpha_k)}{1}\ }(\alpha_k(n)) \ + \ \sum_{k = 3}^{\alpha(n)}\Tleb\left(\alpha_k(n), k\right) \ + \
\sum_{k = 3}^{\alpha(n) - 1}\Tsucc(k)\\
& \ + \runtime_{\alpha_2}(n) \ + \runtime_{\li{nat_size}}(n)
\ + \Tleb(n, 1) \ + \Tleb(n, 3) \ + \Tleb(n, 7)
\end{aligned}
\end{equation*}
In the first summand, $\forall k.~\runtime_{\cdt{(\alpha_k)}{1}\ }(\alpha_k(n)) = \runtime_{\alpha_{k+1}}(n) - \runtime_{\alpha_k}(n)$.
Each $\Tleb$ in the second summand is $\Theta\left(\log_2k\right)$, so the total sum is $\bigO\big(\alpha(n)\log_2\alpha(n)\big)$.
The third summand is $\Theta(\alpha(n))$ by Lemma~\ref{lem: succ-runtime-bin}. The last four summands amount up to $\Theta(\log_2n)$. Therefore,
\begin{equation*}
\begin{aligned}
\runtime_\alpha(n)
& \textstyle = \sum_{k = 2}^{\alpha(n) - 1} \big(\runtime_{\alpha_{k+1}}(n) - \runtime_{\alpha_{k}}(n)\big) + \Theta\left(\log_2 n\right)
= \runtime_{\alpha_{\alpha(n)}}(n) + \Theta\big(\log_2 n\big) \\
& = \Theta\big(\log_2n + 2^{\alpha(n)}\log_2\log_2n \big) + \Theta\big(\log_2n\big)
= \Theta\big(\log_2n\big)
\end{aligned}
\end{equation*}
Therefore this definition of the inverse Ackermann function runs in linear time in terms of the representation's size.
%We bound the RHS by bounding $\runtime\left(\alpha_{\alpha(n)}(n)\right)$, and $\runtime\left(\alpha_i(n)\right)$ in general. Note that by $\alpha_2(n) = \left\lfloor \frac{n - 2}{2} \right\rfloor$, we have $\runtime(\alpha_2, n) \le 2\log_2 n$ for all $n$.
%Note that this bound can potentially be improved by further tightening the above inequalities. Although we do not obtain an exact asymptotic runtime similar 
%to Theorem~\ref{thm: inv-ack-hier-runtime-improved}, since this bound of $4^{\alpha(n)}\log_2n$ is strictly larger than the lower bound of $\log_2n$, it is still extremely small and can be bounded by simpler expressions such as $(\log_2n)^2$.
%Our result is an improved version of the inverse Ackermann function which runs in time $\bigO\big(4^{\alpha(n)}\log_2n\big)$.
%\begin{equation*}
%\alpha(n) \triangleq \begin{cases}
%0 & \text{if } n\le 1\\ 1 & \text{if } 2\le n\le 3 \\ 2 & \text{if } 4\le n\le 7 \\
%\W\alpha\left(\alpha_3,
%\alpha_3(n), 3, n\right) & \text{if } n\ge 8
%\end{cases} \quad \text{ where } \alpha_3 \triangleq \lambda m. \big(\lfloor \log_2(m+2) \rfloor - 2\big)
%\end{equation*}
\subsection{Two-parameter inverse Ackermann function}
Some authors~\cite{chazelle,tarjan} prefer a two-parameter inverse Ackermann function.
\begin{defn} \label{defn: 2para-alpha}
	The two-parameter inverse Ackermann function is defined as:
	\begin{equation} \label{eq: tmp-2para-alpha}
	\ackt (m, n) \triangleq \min\left\{i \ge 1 : \Ack\left(i, \left\lfloor \frac{m}{n} \right\rfloor \right)\ge \log_2n \right\}
	\end{equation}
\end{defn}
Note that $\ackt(n, n)$ and the single-parameter $\alpha(n)$ 
are neither equal nor directly related, but
it is straightforward to modify our techniques to compute $\ackt(m, n)$.
\hide{This function arises from deep runtime analysis of the disjoint-set data structure. Tarjan \cite{tarjan} showed that, in the disjoint-set data structure, the time required $t(m,n)$ for a sequence of $m$ \textsc{\color{magenta}FIND}s intermixed with $n-1$ \textsc{\color{magenta}UNION}s (such that $m \geq n$) is bounded as: $k_{1}m\cdot\alpha(m,n) \leq t(m,n) \leq k_{2}m\cdot\alpha(m,n)$. In graph theory, Chazelle \cite{chazelle} showed that the minimum spanning tree of a connected graph with $n$ vertices and $m$ edges can be found in time $O(m\cdot\alpha(m,n))$. Computing this function is in fact easier than $\alpha(n)$, as when $m$ and $n$ are given, we are reduced to finding the minimum $i\ge 1$ such that $\Ack_i(s)\ge t$ for $s, t$ fixed, which can be done with the following variant of the \emph{inverse Ackermann worker}.
}
\begin{defn} \label{defn: 2para-inv-ack-worker}
	The two-parameter inverse Ackermann worker is a function $\ackt^{\W}$: 
	\hide{$(\mathbb{N}\to \mathbb{N}) \times \mathbb{N}^3\to \mathbb{N}$ such that for all $n, k, b\in \mathbb{N}$ and $f:\mathbb{N}\to \mathbb{N}$:}
	\begin{equation} \label{eq: 2para-inv-ack-worker-recursion}
	\ackt^{\W}(f, n, k, b) = \begin{cases}
	0 & \text{if } b = 0 \vee n\le k \\ 1 + \ackt^{\W}\big(\cdt{f}{1}\circ f , \cdt{f}{1}(n), k, b-1\big) & \text{if } b \ge 1 \wedge n \ge k+1
	\end{cases}
	\end{equation}
\end{defn}
%Similar to the one-parameter version, the following theorem establishes the correct setting for $\W\alpha_2$ to compute $\alpha(m, n)$.
\begin{thm}
	$\displaystyle \ackt(m, n) = 1 + \ackt^{\W}\left(\alpha_1, \alpha_1\big(\lceil\log_2n \rceil\big), \left\lfloor \frac{m}{n} \right\rfloor, \lceil\log_2n \rceil \right)$.
\end{thm}
\hide{
	\begin{proof}[Proof Sketch]
		It is easy to prove in a similar fashion to \cref{lem: inv-ack-worker-intermediate} that for all $n, b, k$ and $i$, if $\alpha_i(n) > k$ and $b > i$, then
		\begin{equation*}
		\W\alpha_2\big(\alpha_1, \alpha_1(b), k, b\big) = i + \W\alpha_2\big(\alpha_{i+1}, \alpha_{i+1}(n), k, b - i\big)
		\end{equation*}
		Now let $k \triangleq \lfloor m/n \rfloor$, $b \triangleq \lceil \log_2n \rceil$ and $l \triangleq \min\big\{i : \alpha_i(b)\le k\big\}$, which exists because $\Ack(i, \cdot)$ increases strictly with $i$. Then $\alpha(m, n) = \max{1, l}$. If $l = 0$, $\alpha_1(b) \le \alpha_0(b) \le k$, so $\W\alpha_2\big(\alpha_1, \alpha_1(b), k, b\big) = 0$, as desired. If $l \ge 1$,
		\begin{equation*}
		1 + \W\alpha_2\big(\alpha_1, \alpha_1(b), k, b\big)
		= 1 + l - 1 + \W\alpha_2\big(\alpha_l, \alpha_l(b), k, b-l\big) = l
		\end{equation*}
		Here $b\ge l$ due to the fact that $\Ack(b, k)\ge b$, so $\alpha_b(b)\le k$. This completes the proof.
\end{proof}}%end hide

\subsection{The value of a linear-time solution to the hierarchy}

Our functions' linear runtime can be understood in two distinct but
complementary ways.  A runtime less than the bitlength is impossible 
without prior knowledge of the size of the input.  Accordingly, in
an information-theory or pure-mathematical sense, our definitions are
optimal up to constant factors.  And of course in practice, linear-time 
solutions are highly useable in real computations.

Sublinear solutions are possible with \emph{a priori} knowledge about
the function and bounds on the inputs one will receive.
An extreme case is $\alpha(n)$, which has value 4 for all practical 
inputs greater than 61: accordingly, 
this function can be inverted in $O(1)$ in practice.  That said, these 
kinds of solutions require external knowledge of the problems and
lookup tables within the algorithm to store the associated precomputed 
values, and so fall more into the realm of engineering than mathematics.

The Coq standard library already includes a linear-time definition 
of division on \li{nat} and \li{N}; and base-two discrete logarithm 
on \li{nat}.  The Mathematical Components library~\cite{MathComp} 
provides a logarithm on \li{nat}.




Other standard libaries
provide some additional ad-hoc examples, such as logarithms in prime
bases


