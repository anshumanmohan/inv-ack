The inverse to the explosively-growing Ackermann function
features in several key algorithmic asymptotic
bounds, such as the union-find data structure~\cite{tarjan} and
the computation of a minimum spanning
tree of a graph~\cite{chazelle}.  Unfortunately, both the Ackermann
function and its inverse
can be hard to understand, and the inverse in particular can be hard to define in a computationally-efficient manner in a theorem prover.  Let us consider why this
is so.
\begin{defn} \label{defn: ack}
The Ackermann-P\'eter function~\cite{peter} (hereafter just
``the'' \linebreak[4] Ackermann function; see~\S\ref{sec:related}) is
written $\mathcal{A} : \mathbb{N}^2 \to \mathbb{N}$ and defined as follows:
\begin{equation}
\label{eq:ackermann}
A(n, m) \triangleq \begin{cases}
m + 1 & \text{ when } n = 0 \\
A(n-1, 1) & \text{ when } n > 0, m = 0 \\
A\big(n-1, A(n, m-1)\big) & \text{ otherwise}
\end{cases}
%A(m, n) \triangleq \begin{cases}
%n + 1 & \text{ if } m = 0 \\
%A(m-1, 1) & \text{ if } m > 0, n = 0 \\
%A(m-1, A(m, n-1)) & \text{ if } m > 0, n > 0
%\end{cases}
\end{equation}
\end{defn}

The one-variable \emph{diagonal} Ackermann function $\Ack : \mathbb{N} \to \mathbb{N}$ is defined as $\Ack(n)~\triangleq~A(n, n)$.	
The diagonal Ackermann function grows explosively: starting from $\Ack(0)$, the first four terms are $1, 3, 7, 61$.  The fifth term is $2^{2^{2^{65536}}}-3$, and the sixth dwarfs the fifth.
%This explosive behavior is problematical when we turn our attention to the canonical definition of
%the inverse Ackermann function\cite{blah}.
This explosive behavior becomes problematical when we consider
the inverse Ackermann function~\cite{chazelle,tarjan}.
\begin{defn} \label{defn: inv_ack}
The inverse Ackermann function $\alpha(n)$ is the smallest~$k$ for
\linebreak which $n \le \Ack(k)$, \emph{i.e.} $\alpha(n) \triangleq \min\left\{k\in \mathbb{N} : n \le \Ack(k)\right\}$.
\end{defn}
This definition is computational: start at $k=0$, calculate $\Ack(k)$,
compare~it~to~$n$, and increment $k$ until $n \le \Ack(k)$.
Unfortunately, its runtime is $\Omega(\Ack(\alpha(n)))$,
so \emph{e.g.} computing $\alpha(100) \mapsto^{*} 4$ in this way requires
$\Ack(4) = 2^{2^{2^{65536}}}-3$ steps!

\subsection{The hyperoperation/Ackermann hierarchy}

%\marginpar{\tiny \color{blue}cite the grit?}
The Ackermann function is relatively easy to define, but hard to
understand.  We see it as
a sequence of $n$-indexed functions $\Ack_n \triangleq \lambda b.A(n,b)$, where for each $n>0$, $\Ack_n$ is the result of applying the previous $\Ack_{n-1}$ $b$ times, with a \note{\emph{kludge}}.

%\marginpar{\tiny \color{blue}Moved Knuth to a footnote because we
%don't discuss it much anymore. It's clear to see that Knuth
%follows from Hyperop.}
The desire to clean up this kludge, and to generalize the natural sequence
of functions (addition, multiplication, exponentiation, \emph{etc.}),
led to the development of hyperoperations\footnote{Knuth arrows~\cite{knuth}, written $a \uparrow^n b$,
are also in the same vein, but we will focus on hyperoperations
since they are more general. In particular, $a \uparrow^n b = a[n+2]b$.}~\cite{goodstein},
written $a [n] b$.
%Hereafter we will refer to this sequence of functions as ``the hierarchy'' when we mean the general pattern rather than \emph{e.g.} specifically ``the Ackermann hierarchy'' or ``the hyperoperation hierarchy''.
Table~\ref{table: hyperop-ack-inv} gives the first five hyperoperations
(indexed by $n$ and named) and the related kludgy $\Ack_n$ Ackermann functions.
We also name and give their inverses,
which we write as $a \angle{n} b$ and $\alpha_n(b)$.
The kludge has three parts. First, the Ackermann hierarchy is related
to the hyperoperation hierarchy with $a$ set to $2$, \emph{i.e.} $2[n]b$;
second, for $n>0$, $\Ack_n$ repeats the previous hyperoperation $2[n-1]b$
%{\color{magenta}(\textbf{not} the previous $\Ack_{n-1}$!)}
three extra times;
lastly, $\Ack_n$ subtracts three\footnote{$\Ack_{1}$ and $\Ack_{2}$ do not break this pattern: $2 + (b + 3) - 3 = 2 + b$, and $2 \cdot (b + 3) - 3 = 2b + 3$.}.

\begin{table}[t]
\begin{centermath}
\begin{array}{c@{\hskip 0.5em}|@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}|@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c}
n & \text{function} & a [n] b & \Ack_n(b) & \text{upper inverse} & a \angle{n} b & \alpha_n(b)\\
\hline
0 & \text{successor} & 1 + b & 1 + b & \text{predecessor} & b - 1 & b - 1 \\
1 & \text{addition} & a + b & 2 + b & \text{subtraction} & b - a & b - 2 \\
2 & \text{multiplication} & a \cdot b & 2b + 3 & \text{division} & \left\lceil \frac{b}{a} \right\rceil & \left\lceil \frac{b-3}{2} \right\rceil \\
3 & \text{exponentiation} & a^b & 2^{b + 3} - 3 & \text{logarithm} & \left\lceil \log_a ~ b \right\rceil & \left\lceil \log_2 ~ (b + 3)\right\rceil - 3 \\
[2pt]
4 & \text{tetration} & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \begin{array}{@{}c@{}}\text{iterated}\\[-3pt]\text{logarithm}\end{array} & \log^*_a ~ b & \log^*_2 ~ (b + 3) - 3
\end{array}
\end{centermath}
\caption{Hyperoperations, the Ackermann hierarchy, and inverses thereof}
\label{table: hyperop-ack-inv}
\vspace{-2em}
\end{table}

It is worth studying and inverting the hyperoperations before handling the Ackermann inverses.
Once we have defined these inverses, we shall define an efficient inverse $\alpha$ to the diagonal Ackermann $\Ack$ thanks to our Theorem \ref{thm: inv-ack-new},
which characterizes $\alpha$ without referring to $\Ack$ directly:
%\[
$
\forall n.~ \alpha(n) = \min\big\{k : \alpha_k(n)\le k \big\}
$.
%\]

%Our initial goal of inverting the Ackermann function can thus be broken
%into three parts: first, inverting each individual member of the hyperoperation hierarchy; second, using these individual inverses to invert the diagonal hyperoperation {\color{red}$a [n] n$}; and lastly adjusting for the kludge.

%The kludge seems to be a conjugacy between the Ackermann hierarchy and the hyperoperations at $a=2$. For each $n$, we note $\Ack_n = \phi^{-1}\circ \left(2[n]\right)\circ\phi$ where $\phi(n) = n+3$.\footnote{It might appear that $\Ack_{1}$ and $\Ack_{2}$ break this pattern, but they do not since $2 + (b + 3) - 3 = 2 + b$ and $2 \cdot (b + 3) - 3 = 2b + 3$.}


%\[
%\begin{array}{c|cccc|c}
%n & \quad a [n] b \quad  & \quad a \uparrow^{n-2} b \quad & \quad \Ack(n,b) \quad & \text{function} & \text{inverse} \\
%\hline
%0 & 1 + b & {-} & 1 + b & \text{successor} & \text{predecessor} \\
%1 & a + b & {-} & 2 + b & \text{addition} & \text{subtraction} \\
%2 & a \cdot b & a \cdot b & 2b + 3 & \text{multiplication} & \text{division} \\
%3 & a^b & a^b & 2^{b + 3} - 3 & \text{exponentiation} & \text{logarithm} \\
%[2pt]
%4 & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \text{tetration} & \begin{array}{@{}c@{}}\text{iterated}\\[-2pt]\text{logarithm}\end{array}
%\end{array}
%\]

\subsection{Increasing functions and their inverses}
\label{sec:incfuncinv}
Defining increasing functions is often easier than defining their inverses.
For instance, addition, multiplication, and exponentiation on Church numerals
are simpler than subtraction, division, and logarithm. Similarly, defining multiplication in Gallina~\cite{coq} is easy, but defining division is unexpectedly painful:
%Defining increasing functions is often significantly easier than defining their inverses.
%The Church numeral encodings of addition, multiplication, and exponentiation
%are each simpler than their corresponding inverses, \emph{i.e.} subtraction, division, and logarithm. Similarly, defining multiplication in Gallina~\cite{coq} is easy, but defining division is unexpectedly painful:

\begin{minipage}[c]{0.4\textwidth}
% \vspace{-1.5em}
\begin{tabular}{@{}l@{}@{\qquad \qquad}l}
\begin{lstlisting}
Fixpoint mult a b :=
  match a with
  | 0 => 0
  | S a' => b + mult a' b
  end.
\end{lstlisting}
&
\begin{lstlisting}
Fixpoint div a b :=
  match a with
  | 0 => 0
  | _ => 1 + div (a - b) b
  end.
\end{lstlisting}
\end{tabular} \\[5pt]
\end{minipage}

\noindent Coq accepts the definition of \li{mult}; indeed this
is how multiplication is defined in the standard library.  The function
\li{div} should calculate multiplication's upper inverse,
\emph{i.e.} $\li{div}~x~y \mapsto^{*} \lceil \frac{x}{y} \rceil$, but the definition
is rejected by Coq's termination checker.  Coq worries that
\li{a - b} might not be structurally smaller than \li{a}, since
subtraction is ``just another function,'' and is thus treated opaquely. Indeed, Coq
is right to be nervous: \li{div} will not terminate
when $\li{a}>0$ and $\li{b}=0$.

Of course, division \emph{can} be defined, but an elegant definition is a little
subtle. We certainly need to do more than just check that $\li{b}>0$.
Two standard techniques are to define a custom termination measure~\cite{chlipala};
and to define a straightforward function augmented with an
extra \li{nat} parameter that denotes a ``gas'' value that decreases at each recursive
call~\cite{gasperson}.  Both techniques are vaguely unsatisfying and neither is ideal
for our purposes: the first can be hard to generalize and the second requires a method to calculate the appropriate amount of gas.  For instance, calculating the amount of gas to compute $\alpha(100)$ the
``canonical'' way, \emph{e.g.} at least $2^{2^{2^{65536}}}$, is problematic for many reasons,
not least because we cannot use the inverse Ackerman function in its own termination argument.
%One method is to define a custom termination measure~\cite{Chlipala?}, but this is
%both vaguely unsatisfying and not always easy to generalize.

Realizing this, the standard library employs
a cleverer approach to define division, but we are not aware of any explanation
of the technique used, and we also find it hard to extend that technique
to other functions in the hierarchy.  One indication
of this difficulty is that the Coq standard library does not include a $\mathsf{log}_b$ function\footnote{Coq's standard library does include a $\mathsf{log}_2$ function, but
change-of-base does not work on discrete logarithms:
$\left \lfloor \frac{\lfloor \mathsf{log}_2 100 \rfloor}{\lfloor \mathsf{log}_2 7 \rfloor} \right \rfloor = 3 \; \not= \; 2 = \lfloor \mathsf{log}_7 100 \rfloor$.
%{\color{blue} efficiency of standard library version?} {\color{red} SSReflect?}
}, to say nothing of a $\mathsf{log}^{*}_b$ or the inverse Ackermann.
%We discuss alternative approaches further in \cref{sec:related}.

\subsection{Contributions}
%\marginpar{Let's carefully double-check this section once the rest of the paper is close to final form.}
We provide a complete solution to inverting each individual function in the hyperoperation/Ackermann hierarchy,
as well as the diagonal Ackermann function itself.  All our functions are structurally recursive, so
Coq is immediately convinced of their termination.  Moreover, all our functions run in linear time: practical utility follows from theoretical grace.
Finally, our techniques are compact: consider the 14 lines of code that invert the diagonal Ackermann function in \li{nat} on the 18th page of this paper (\S\ref{apx:code}).  Our Coq development is about 2,500 lines, split between parallel efforts in unary-encoded and binary-encoded inputs.
%The rest of this paper is organized as follows.
%, so they are as asymptotically efficient as they can be, and are thus suitable for extraction\footnote{A criterion more useful in practice for $\mathsf{log}_b$ and perhaps $\mathsf{log}^*_b$ than the other members of the hierarchy.}
%\begin{itemize}
%\item[\S\ref{blah}] We explain our core techniques of \emph{repeaters} and \emph{countdowns}. {\color{red}that allow us to define each level of the Ackermann hierarchy---and their upper inverses---in a straightforward and uniform manner.} We show how countdowns, in particular, can be written
%    structurally recursively.
%\item[\S\ref{blah}] We show how to use our techniques to define the Ackermann and upper inverse Ackermann functions themselves. {\color{red}Add inverse hyperop?}
%\item[\S\ref{blah}] We detail a few optimizations that improve the running time of our individual hierarchy inverse functions from $O(n^2)$ to $O(n)$, and then further improve the running time of our inverse Ackermann function from $O\big(n \cdot \alpha(n)\big)$ to $O(n)$.
%\item[\S\ref{blah}] We extend our functions in several useful ways: to the two-argument inverse Ackermann, lower inverses, and binary representations.
%\item[\S\ref{blah}] We discuss related work and give some closing thoughts.
%\end{itemize}
\begin{itemize}
	\item[\S\ref{sec: countdown-repeater}] We show a formal definition for hyperoperations, present Coq encodings for hyperoperations and the Ackermann function, and discuss \emph{repeater}.
	\item[\S\ref{sec: countdown}] We discuss upper inverses and show how to invert repeater with \emph{countdown}.
	\item[\S\ref{sec: inv-hyperop}] We use our techniques to define the inverse hyperoperations, whose notable members include division, logarithm and iterated logarithm with arbitrary base. We then sketch a method to find the inverse Ackermann function.
	\item[\S\ref{sec: inv-ack}] We give a computation for the inverse Ackermann function in $\bigO(n)$ time.
	\item[\S\ref{sec: binary}] We extend to $\bigO(b)$, where $b$ is the bitlength
	of the binary encoding of $n$.
	\item[\S\ref{sec: discussion}] We discuss the value of linear time inverses for
explosively growing functions, and extend to the two-argument inverse Ackermann function. %, \note{other derivative functions such as foo bar doo dah,} and survey related work.
\end{itemize}
All our techniques are mechanized in Coq and our code is available
online~\cite{inv-ack}. Further, definitions and theorems presented
in this paper are \href{https://github.com/inv-ack/inv-ack}{hyperlinked}
to appropriate points in our codebase. Interested readers are invited to also
explore the proofs of correctness and associated lemmas that were elided in this paper.
%, and
%each theorem and definition is hyperlinked (in {\color{cyan}cyan}) to our Coq development for %browsing.

