The inverse to the
explosively-growing Ackermann function features in several key algorithmic asymptotic
bounds, such as the union-find data structure~\cite{tarjan} and computing a minimum spanning
tree of a graph~\cite{chazelle}.  Unfortunately, both the Ackermann function and its inverse
can be hard to understand, and the inverse in particular can be hard to define in a computationally-efficient manner in a theorem prover.  Let us consider why this
is so.
\begin{defn} \label{defn: ack}
The \href{https://github.com/anshumanmohan/inv-ack/blob/fb8c1f625ccae7cf0c3012d926dbbf3972529d74/applications.v#L111-L120}{Ackermann-P\'eter function}~\cite{blah} (hereafter just ``the'' Ackermann function; see \cref{sec:related}) is a recursive two-variable function $\text{A} : \mathbb{N}^2 \to \mathbb{N}$ defined as follows:
\begin{equation}
\label{eq:ackermann}
A(n, m) \triangleq \begin{cases}
m + 1 & \text{ when } n = 0 \\
A(n-1, 1) & \text{ when } n > 0, m = 0 \\
A\big(n-1, A(n, m-1)\big) & \text{ otherwise}
\end{cases}
%A(m, n) \triangleq \begin{cases}
%n + 1 & \text{ if } m = 0 \\
%A(m-1, 1) & \text{ if } m > 0, n = 0 \\
%A(m-1, A(m, n-1)) & \text{ if } m > 0, n > 0
%\end{cases}
\end{equation}
\end{defn}

The one-variable \emph{diagonal} Ackermann function $\Ack : \mathbb{N} \to \mathbb{N}$ is defined as $\Ack(n)~\triangleq~\Ack(n, n)$.	
The diagonal Ackermann function grows explosively: starting from $\Ack(0)$, the first four terms are $1, 3, 7, 61$.  The fifth term is $2^{2^{2^{65536}}}-3$, and the sixth dwarfs the fifth.
%This explosive behavior is problematical when we turn our attention to the canonical definition of
%the inverse Ackermann function\cite{blah}.
This explosive behavior becomes problematical when we consider
the inverse Ackermann function\cite{blah}.
\begin{defn} \label{defn: inv_ack}
The inverse Ackermann function $\alpha(n)$ is canonically defined as the minimum $k$ for which $n \le \Ack(k)$, \emph{i.e.} $\alpha(n) \triangleq \min\left\{k\in \mathbb{N} : n \le \Ack(k)\right\}$.
\end{defn}
In a sense this definition is computational: starting with $k=0$, calculate $\Ack(k)$, compare
it to $n$, and then increment $k$ until $n \le \Ack(k)$.
Unfortunately, the running time of this algorithm is $\Omega(\Ack(\alpha(n)))$,
so \emph{e.g.} computing $\alpha(100) \mapsto^{*} 4$ in this way requires
$\Ack(4) = 2^{2^{2^{65,536}}}$ steps!

\subsection{The hyperoperation/Ackermann hierarchy}

\marginpar{\tiny \color{blue}cite the grit?}
The Ackermann function is relatively simple to define, but a little hard to 
understand.  {\color{magenta}We think of it as} 
a sequence of $n$-indexed functions $\Ack_n \triangleq \lambda b.\Ack(n,b)$, where for each $n>0$, $\Ack_n$ is the result of applying the previous $\Ack_{n-1}$ $b$ times, with a \emph{kludge}.

\marginpar{\tiny \color{blue}Moved Knuth to a footnote because we 
don't discuss it much anymore. It's clear to see that Knuth 
follows from Hyperop.}
The desire to clean up this kludge, and to generalize the natural sequence
of functions ``addition,'' ``multiplication,'' ``exponentiation,'' \ldots,
lead us to explore the related idea of hyperoperations~\cite{blah},
written $a [n] b$.\footnote{Knuth arrows~\cite{blah}, written $a \uparrow^n b$,
are also in the same vein, but we will focus on hyperoperations 
since they are more general. In particular, $a \uparrow^n b = a[n+2]b$.}
%Hereafter we will refer to this sequence of functions as ``the hierarchy'' when we mean the general pattern rather than \emph{e.g.} specifically ``the Ackermann hierarchy'' or ``the hyperoperation hierarchy''.
\Cref{table: hyperop-ack-inv} illustrates this relation and the Ackermann kludge
by {\color{magenta}showing the first 5 hyperoperations (indexed by $n$ and named), 
along with their relation to $\Ack_n$ functions, and their inverses.}
\begin{table}[h]
	\[
\begin{array}{c|cc|c|cc}
n & \text{function} & \quad a [n] b \quad & \quad \Ack_n(b) \quad & \text{inverse} \\
\hline
0 & \text{successor} & 1 + b & 1 + b & \text{predecessor} & b - 1 \\
1 & \text{addition} & a + b & 2 + b & \text{subtraction} & b - a \\
2 & \text{multiplication} & a \cdot b & 2b + 3 & \text{division} & \frac{b}{a}\\
3 & \text{exponentiation} & a^b & 2^{b + 3} - 3 & \text{logarithm} & \mathsf{log}_a b \\
[2pt]
4 & \text{tetration} & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \begin{array}{@{}c@{}}\text{iterated}\\[-3pt]\text{logarithm}\end{array} & \mathsf{log}^*_a b
\end{array}
\] \caption{Hyperoperations, Ackermann functions and inverse.}
\label{table: hyperop-ack-inv}
\end{table}

\marginpar{\tiny \color{blue}I think the ``not the previous $\Ack_{n-1}$!'' is confusing 
and can just be removed.}
The kludge has three parts. First, the Ackermann hierarchy is related 
to the hyperoperation hierarchy with $a=2$, ie $2[n]b$; 
second, for $n>0$, $\Ack_n$ repeats the previous hyperoperation $2[n-1]b$
{\color{magenta}(\textbf{not} the previous $\Ack_{n-1}$!)} three extra times; 
lastly, $\Ack_n$ subtracts three.\footnote{$\Ack_{1}$ and $\Ack_{2}$ do not break this pattern: $2 + (b + 3) - 3 = 2 + b$, and $2 \cdot (b + 3) - 3 = 2b + 3$.}
It is worth studying and inverting the hyperoperations before handling the Ackermann function.

%Our initial goal of inverting the Ackermann function can thus be broken
%into three parts: first, inverting each individual member of the hyperoperation hierarchy; second, using these individual inverses to invert the diagonal hyperoperation {\color{red}$a [n] n$}; and lastly adjusting for the kludge.

%The kludge seems to be a conjugacy between the Ackermann hierarchy and the hyperoperations at $a=2$. For each $n$, we note $\Ack_n = \phi^{-1}\circ \left(2[n]\right)\circ\phi$ where $\phi(n) = n+3$.\footnote{It might appear that $\Ack_{1}$ and $\Ack_{2}$ break this pattern, but they do not since $2 + (b + 3) - 3 = 2 + b$ and $2 \cdot (b + 3) - 3 = 2b + 3$.}


%\[
%\begin{array}{c|cccc|c}
%n & \quad a [n] b \quad  & \quad a \uparrow^{n-2} b \quad & \quad \Ack(n,b) \quad & \text{function} & \text{inverse} \\
%\hline
%0 & 1 + b & {-} & 1 + b & \text{successor} & \text{predecessor} \\
%1 & a + b & {-} & 2 + b & \text{addition} & \text{subtraction} \\
%2 & a \cdot b & a \cdot b & 2b + 3 & \text{multiplication} & \text{division} \\
%3 & a^b & a^b & 2^{b + 3} - 3 & \text{exponentiation} & \text{logarithm} \\
%[2pt]
%4 & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \text{tetration} & \begin{array}{@{}c@{}}\text{iterated}\\[-2pt]\text{logarithm}\end{array}
%\end{array}
%\]

\subsection{Increasing functions and their inverses}
Defining increasing functions is often significantly simpler than defining their inverses.
The Church numeral encodings of addition, multiplication, and even exponentiation
are each simpler than their corresponding inverses of subtraction, division, and logarithm. Similarly, defining multiplication in Gallina is trivial, but defining division is unexpectedly painful.\\[5pt]

\hide{
\begin{minipage}[c]{0.4\textwidth}
\begin{lstlisting}
Fixpoint mult a b :=
 match a with
 | 0 => 0
 | S a' => b + mult a' b
 end.
\end{lstlisting}
\end{minipage}
\vline
\begin{minipage}[c]{0.4\textwidth}
\begin{lstlisting}
Fixpoint div a b :=
 match a with
 | 0 => 0
 | _ => 1 + div (a - b) b
 end.
\end{lstlisting}
\end{minipage}
} % This is an alternate option that has its own aesthetic issues

\begin{tabular}{@{}l@{~~~}|@{~~~}l}
\begin{lstlisting}
Fixpoint mult a b :=
 match a with
 | 0 => 0
 | S a' => b + mult a' b
 end.
\end{lstlisting}
&
\begin{lstlisting}
Fixpoint div a b :=
 match a with
 | 0 => 0
 | _ => 1 + div (a - b) b
 end.
\end{lstlisting}
\end{tabular} \\[5pt]
The definition of \li{mult} is of course accepted immediately by Coq; indeed
it is the precise way multiplication is defined in the standard library.  The function
\li{div} should calculate multiplication's upper inverse,
\emph{i.e.} $\li{div}~x~y \mapsto^{*} \lceil \frac{x}{y} \rceil$, but the definition
is rejected by the termination checker.  Coq worries that
\li{a - b} might not be structurally smaller than \li{a}, since
subtraction is ``just another function,'' and is thus treated opaquely. Indeed, Coq
is right to be nervous: \li{div} will not terminate
when $\li{a}>0$ and $\li{b}=0$.

Of course, division \emph{can} be defined, but an elegant definition is a little
subtle---certainly, we need to do more than just check that $\li{b}>0$.
Two standard techniques are to define a custom termination measure~\cite{Chlipala?};
and to define a straightforward function augmented with an
extra \li{nat} parameter that denotes a ``gas'' value that decreases at each recursive
call~\cite{something}.  Both techniques are vaguely unsatisfying and neither is ideal
for our purposes: the first can be hard to generalize and the second requires a method to calculate the appropriate amount of gas.  Calculating the amount of gas to compute $\alpha(100)$ the
``canonical'' way, \emph{e.g.} at least $2^{2^{2^{65,536}}}$, is problematic for many reasons,
not least because we cannot use the inverse Ackerman function in its own termination argument.
%One method is to define a custom termination measure~\cite{Chlipala?}, but this is
%both vaguely unsatisfying and not always easy to generalize.
Realizing this, the standard library employs
a cleverer approach to define division, but {\color{red} we are not aware of any explanation of the technique used, and we also find it hard to extend that technique to other members of the hierarchy}.  One indication
of this difficulty is that the Coq standard library does not include a $\mathsf{log}_b$ function\footnote{Coq's standard library \textbf{does} include a $\mathsf{log}_2$ function, but
change-of-base does not work on \li{nat}:
$\left \lfloor \frac{\lfloor \mathsf{log}_2 100 \rfloor}{\lfloor \mathsf{log}_2 7 \rfloor} \right \rfloor = 3 \; \not= \; 2 = \lfloor \mathsf{log}_7 100 \rfloor$.  {\color{blue} efficiency of standard library version?} {\color{red} SSReflect?}}. We discuss alternative approaches further in \cref{sec:related}.

\subsection{Contributions}
\marginpar{Let's carefully double-check this section once the rest of the paper is close to final form.} We provide a complete solution to inverting each individual function in the Ackermann hierarchy,
as well as the Ackermann function itself.  All our functions are structurally recursive, so
Coq is immediately convinced of their termination.  Moreover, all our functions run in linear
time (in the size of a number's representation in unary or binary), so they are as asymptotically efficient as they can be, and are thus suitable for extraction\footnote{A criterion more useful in practice for $\mathsf{log}_b$ and perhaps $\mathsf{log}^*_b$ than the other members of the hierarchy.}.  Finally, our techniques are succinct: the code to invert the Ackermann function fits in a single page of this paper, and our entire Coq development is only {\color{red} X} lines. The rest of this paper is organized as follows.
%\begin{itemize}
%\item[\S\ref{blah}] We explain our core techniques of \emph{repeaters} and \emph{countdowns}. {\color{red}that allow us to define each level of the Ackermann hierarchy---and their upper inverses---in a straightforward and uniform manner.} We show how countdowns, in particular, can be written
%    structurally recursively.
%\item[\S\ref{blah}] We show how to use our techniques to define the Ackermann and upper inverse Ackermann functions themselves. {\color{red}Add inverse hyperop?}
%\item[\S\ref{blah}] We detail a few optimizations that improve the running time of our individual hierarchy inverse functions from $O(n^2)$ to $O(n)$, and then further improve the running time of our inverse Ackermann function from $O\big(n \cdot \alpha(n)\big)$ to $O(n)$.
%\item[\S\ref{blah}] We extend our functions in several useful ways: to the two-argument inverse Ackermann, lower inverses, and binary representations.
%\item[\S\ref{blah}] We discuss related work and give some closing thoughts.
%\end{itemize}
{\color{magenta}
\begin{itemize}
	\item[\S\ref{sec: countdown-repeater}] Starting from hyperoperations, we discuss the concepts \emph{repeater} and \emph{expansions}, and how the need for their inverse leads to \emph{countdowns}.
	\item[\S\ref{sec: inv-hyperop}] We show how to use our techniques to define the inverse hyperoperations, whose notable members include division, logarithm and iterated logarithm with arbitrary base; and sketch a method to find the Inverse Ackermann function.
	\item[\S\ref{sec: inv-ack}] We detail a specialized computation for the Inverse Ackermann function, improving over the sketch's running time of $\bigO\big(n^2\big)$ to $\bigO(n)$.
	\item[\S\ref{sec: discussion}] We extend our functions in several useful ways: to the two-argument inverse Ackermann, lower inverses, and binary representations.
\end{itemize}}
All of our techniques are mechanized in Coq~\cite{blah}, and 
each theorem and definition is hyperlinked (in {\color{cyan}cyan}) to our Coq development for browsing.

