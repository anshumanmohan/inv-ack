The inverse to the
explosively-growing Ackermann function features in several key algorithmic asymptotic
bounds, such as the union-find data structure~\cite{tarjan} and computing a minimum spanning 
tree of a graph~\cite{chazelle}.  Unfortunately, both the Ackermann function and its inverse 
can be hard to understand, and the inverse in particular can be hard to define in a computationally-efficient manner in a theorem prover.  Let us consider why this
is so.
\begin{defn} \label{defn: ack}
The Ackermann-P\'eter function~\cite{blah} (hereafter, just the Ackermann function) is a recursive two-variable
function $\text{A} : \mathbb{N}^2 \to \mathbb{N}$:
\begin{equation}
A(m, n) \triangleq \begin{cases}
n + 1 & \text{ if } m = 0 \\
A(m-1, 1) & \text{ if } m > 0, n = 0 \\
A(m-1, A(m, n-1)) & \text{ if } m > 0, n > 0
\end{cases}
\end{equation}
The diagonal Ackermann function $\Ack(n)$ is then defined by $\Ack(n) \triangleq \Ack(n, n)$.
\end{defn}
The diagonal Ackermann function grows explosively: starting from $\Ack(0)$, the first four terms are $1, 3, 7, 61$.  The fifth term is $2^{2^{2^{65,536}}} - 3$, and the sixth dwarfs the fifth.
This explosive behavior is problematical when we turn our attention to the canonical definition of 
the inverse Ackermann function\cite{blah}.
\begin{defn} \label{defn: inv_ack}
The inverse Ackermann function $\alpha(n)$ is canonically defined as the minimum $k$ for which $n \le \Ack(k)$, \emph{i.e.} $\alpha(n) \triangleq \min\left\{k\in \mathbb{N} : n \le \Ack(k)\right\}$.
\end{defn}
In a sense this definition is computational: starting with $k=0$, calculate $\Ack(k)$, compare
it to $n$, and then increment $k$ until $n \le \Ack(k)$.  Unfortunately, the running time of this algorithm is $\Omega(\Ack(\alpha(n)))$, so to compute, for example, $\alpha(100) \mapsto^{*} 4$ in this way requires more than $\Ack(4) = 2^{2^{2^{65,536}}}$ steps!

\paragraph{The hyperoperation/Knuth/Ackermann hierarchy}

The Ackermann function is relatively simple to define, but a little hard to understand.  We 
think of it as a sequence of $n$-indexed functions $\Ack_n \triangleq \lambda b.\Ack(n,b)$, where for each $n>0$, $\Ack_n$ iterates over the previous $\Ack_{n-1}$ $b$ times,
\emph{with a kludge}.

The desire to clean up this kludge, as well as generalize the natural sequence of functions ``addition,'' ``multiplication,'' ``exponentiation,'' \ldots, lead to the development of two
related ideas: hyperoperations~\cite{blah}, written $a [n] b$, and Knuth arrows~\cite{blah}, written
$a \uparrow^n b$.  Since hyperoperations are a little more general (in particular, $a \uparrow^n b = a[n+2]b)$, we will focus on them.  Hereafter we will refer to this sequence of functions as ``the hierarchy'' when we mean the general pattern rather than \emph{e.g.} specifically ``the Ackermann hierarchy'' or ``the hyperoperation hierarchy''.  To illustrate the pattern, and demonstrate the Ackermann kludge, the following table contains the first 5 hyperations (indexed by $n$ and named), along with how they relate to the $\Ack_n$ functions, and their inverses:
\[
\begin{array}{cc|cc|cc}
n & \text{function} & \quad a [n] b \quad & \quad \Ack_n(b) \quad & \text{inverse} \\
\hline
0 & \text{successor} & 1 + b & 1 + b & \text{predecessor} & b - 1 \\
1 & \text{addition} & a + b & 2 + b & \text{subtraction} & b - a \\
2 & \text{multiplication} & a \cdot b & 2b + 3 & \text{division} & \frac{b}{a}\\
3 & \text{exponentiation} & a^b & 2^{b + 3} - 3 & \text{logarithm} & \mathsf{log}_a b \\
[2pt]
4 & \text{tetration} & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \begin{array}{@{}c@{}}\text{iterated}\\[-3pt]\text{logarithm}\end{array} & \mathsf{log}^*_a b 
\end{array}
\]
The kludge has three parts.  First, the Ackermann hierarchy is related to the hyperoperation hierarchy when $a=2$; second, for $n>0$, $\Ack_n$ repeats the previous hyperoperation (\textbf{not} the previous $\Ack_{n-1}$!) three extra times; lastly, $\Ack_n$ subtracts three\footnote{It might appear that $\Ack_{1}$ and $\Ack_{2}$
break this pattern, but they do not since $2 + (b + 3) - 3 = 2 + b$ and $2 \cdot (b + 3) - 3 = 2b + 3$.}.  Our initial goal of inverting the Ackermann function can thus be broken
into three parts: first, inverting each individual member of the hyperoperation hierarchy; second, using these individual inverses to invert the diagonal hyperoperation {\color{red}$a [n] n$}; and lastly adjusting for the kludge.

%\[
%\begin{array}{c|cccc|c}
%n & \quad a [n] b \quad  & \quad a \uparrow^{n-2} b \quad & \quad \Ack(n,b) \quad & \text{function} & \text{inverse} \\
%\hline
%0 & 1 + b & {-} & 1 + b & \text{successor} & \text{predecessor} \\
%1 & a + b & {-} & 2 + b & \text{addition} & \text{subtraction} \\
%2 & a \cdot b & a \cdot b & 2b + 3 & \text{multiplication} & \text{division} \\
%3 & a^b & a^b & 2^{b + 3} - 3 & \text{exponentiation} & \text{logarithm} \\
%[2pt]
%4 & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \text{tetration} & \begin{array}{@{}c@{}}\text{iterated}\\[-2pt]\text{logarithm}\end{array}
%\end{array}
%\]

\paragraph{Increasing functions and their inverses}
Defining increasing functions is often significantly simpler than defining their inverses.
The Church numeral encodings of addition, multiplication, and even exponentiation
are each simpler than their corresponding inverses of subtraction, division, and logarithm. We
see the same pattern in mechanized contexts: defining multiplication in Gallina is
child's play, but defining division is unexpectedly painful:
\begin{tabular}{@{}l@{~~~}|@{~~~}l}
\begin{lstlisting}
Fixpoint mult a b :=
 match a with
 | 0 => 0
 | S a' => b + mult a' b
 end.
\end{lstlisting}
&
\begin{lstlisting}
Fixpoint div a b :=
 match a with
 | 0 => 0
 | _ => 1 + div (a - b) b
 end.
\end{lstlisting}
\end{tabular} \\
The definition of \li{mult} is of course accepted immediately by Coq; indeed
it is the precise way multiplication is defined in the standard library.  The function
\li{div} should calculate multiplication's upper inverse,
\emph{i.e.} $\li{div}~x~y \mapsto^{*} \lceil \frac{x}{y} \rceil$, but the definition
is rejected by the termination checker.  Coq worries that
\li{a - b} might not be structurally smaller than \li{a}, since
subtraction is ``just another function,'' and is thus treated opaquely.  And indeed Coq
is right to be nervous, since \li{div} will not in fact terminate
when $\li{a}>0$ and $\li{b}=0$.

Of course, division \emph{can} be defined, but an elegant definition is a little
subtle---certainly, something beyond merely checking that $\li{b}>0$ is needed.
One method is to define a custom termination measure~\cite{Chlipala?}, but this is
both vaguely unsatisfying and not always easy to generalize.  The standard library employs
a cleverer approach to define division, but {\color{red} we are not aware of any explanation of the technique used, nor is it trivial to extend to other members of the hierarchy}.  One indication
that this is so is that the Coq standard library does not include a $\mathsf{log}_b$ function\footnote{Coq's standard library \textbf{does} include a $\mathsf{log}_2$ function, but 
change-of-base does not work on {\color{red} nat}:
$\left \lfloor \frac{\lfloor \mathsf{log}_2 100 \rfloor}{\lfloor \mathsf{log}_2 7 \rfloor} \right \rfloor = 3 \not = 2 = \lfloor \mathsf{log}_7 100 \rfloor$.  {\color{blue} efficiency of standard library version?} {\color{red} SSReflect?}}.

\paragraph{Contributions}
We provide a complete solution to inverting each individual function in the Ackermann hierarchy,
as well as the Ackermann function itself.  All of our functions are structurally recursive, so
Coq is immediately convinced of their termination.  Moreover, all of our functions run in linear
time (in the size of a number's representation in unary or binary), so they are as asymptotically efficient as they can be, and thus suitable for extraction\footnote{A criterion more useful in practice for $\mathsf{log}_b$ and perhaps $\mathsf{log}^*_b$ than the other members of the hierarchy, to be sure.}.  Finally, our techniques are succinct: the code to invert the Ackermann function fits onto a single page of this paper, and our entire Coq development is only {\color{red} X} lines. The rest of this paper is organized as follows.
\begin{itemize}
\item[\S\ref{blah}] We explain our core techniques of \emph{repeaters} and \emph{countdowns}. {\color{red}that allow us to define each level of the Ackermann hierarchy---and their upper inverses---in a straightforward and uniform manner.} We show how countdowns, in particular, can be written
    structurally recursively.
\item[\S\ref{blah}] We show how to use our techniques to define the Ackermann and upper inverse Ackermann functions themselves. {\color{red}Add inverse hyperop?}
\item[\S\ref{blah}] We detail a few optimizations that improve the running time of our individual hierarchy inverse functions from $O(n^2)$ to $O(n)$, and then further improve the running time of our inverse Ackermann function from $O\big(n \cdot \alpha(n)\big)$ to $O(n)$.
\item[\S\ref{blah}] We extend our functions in several useful ways: to the two-argument inverse Ackermann, lower inverses, and binary representations.
\item[\S\ref{blah}] We discuss related work and give some closing thoughts.
\end{itemize}
All of our techniques are mechanized in Coq~\cite{blah}, and for online readers, 
each theorem and definition is hyperlinked to our Coq development for browsing.

