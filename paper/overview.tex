The inverse to the
explosively-growing Ackermann function features in several key algorithmic asymptotic
bounds, such as the union-find data structure~\cite{tarjan} and computing a minimum spanning
tree of a graph~\cite{chazelle}.  Unfortunately, both the Ackermann function and its inverse
can be hard to understand, and the inverse in particular can be hard to define in a computationally-efficient manner in a theorem prover.  Let us consider why this
is so.
\begin{defn} \label{defn: ack} 
The Ackermann-P\'eter function~\cite{peter} (hereafter just ``the'' Ackermann function; see~\S\ref{sec:related}) is a recursive two-variable function $\text{A} : \mathbb{N}^2 \to \mathbb{N}$ defined as follows:
\begin{equation}
\label{eq:ackermann}
A(n, m) \triangleq \begin{cases}
m + 1 & \text{ when } n = 0 \\
A(n-1, 1) & \text{ when } n > 0, m = 0 \\
A\big(n-1, A(n, m-1)\big) & \text{ otherwise}
\end{cases}
%A(m, n) \triangleq \begin{cases}
%n + 1 & \text{ if } m = 0 \\
%A(m-1, 1) & \text{ if } m > 0, n = 0 \\
%A(m-1, A(m, n-1)) & \text{ if } m > 0, n > 0
%\end{cases}
\end{equation}
\end{defn}

The one-variable \emph{diagonal} Ackermann function $\Ack : \mathbb{N} \to \mathbb{N}$ is defined as $\Ack(n)~\triangleq~\Ack(n, n)$.	
The diagonal Ackermann function grows explosively: starting from $\Ack(0)$, the first four terms are $1, 3, 7, 61$.  The fifth term is $2^{2^{2^{65536}}}-3$, and the sixth dwarfs the fifth.
%This explosive behavior is problematical when we turn our attention to the canonical definition of
%the inverse Ackermann function\cite{blah}.
This explosive behavior becomes problematical when we consider
the inverse Ackermann function\cite{chazelle, tarjan}.
\begin{defn} \label{defn: inv_ack}
The inverse Ackermann function $\alpha(n)$ is defined as the minimum $k$ for which $n \le \Ack(k)$, \emph{i.e.} $\alpha(n) \triangleq \min\left\{k\in \mathbb{N} : n \le \Ack(k)\right\}$.
\end{defn}
In a sense this definition is computational: starting with $k=0$, calculate $\Ack(k)$, compare
it to $n$, and then increment $k$ until $n \le \Ack(k)$.
Unfortunately, the running time of this algorithm is $\Omega(\Ack(\alpha(n)))$,
so \emph{e.g.} computing $\alpha(100) \mapsto^{*} 4$ in this way requires
$\Ack(4) = 2^{2^{2^{65,536}}}$ steps!

\subsection{The hyperoperation/Ackermann hierarchy}

%\marginpar{\tiny \color{blue}cite the grit?}
The Ackermann function is relatively straightforward to define, but a little hard to
understand.  We think of it as
a sequence of $n$-indexed functions $\Ack_n \triangleq \lambda b.\Ack(n,b)$, where for each $n>0$, $\Ack_n$ is the result of applying the previous $\Ack_{n-1}$ $b$ times, with a \emph{kludge}.

%\marginpar{\tiny \color{blue}Moved Knuth to a footnote because we
%don't discuss it much anymore. It's clear to see that Knuth
%follows from Hyperop.}
The desire to clean up this kludge, and to generalize the natural sequence
of functions ``addition,'' ``multiplication,'' ``exponentiation,'' \emph{etc.},
led to the development of the related idea of hyperoperations\footnote{Knuth arrows~\cite{knuth}, written $a \uparrow^n b$,
are also in the same vein, but we will focus on hyperoperations
since they are more general. In particular, $a \uparrow^n b = a[n+2]b$.}~\cite{goodstein},
written $a [n] b$.
%Hereafter we will refer to this sequence of functions as ``the hierarchy'' when we mean the general pattern rather than \emph{e.g.} specifically ``the Ackermann hierarchy'' or ``the hyperoperation hierarchy''.
\Cref{table: hyperop-ack-inv} illustrates this relation and the Ackermann kludge
by showing the first five hyperoperations (indexed by $n$ and named),
along with their relation to the $\Ack_n$ functions, and their inverses.

\begin{table}[t]
\begin{centermath}
\begin{array}{c|cc|c|cc}
n & \text{function} & a [n] b & \Ack_n(b) & \text{inverse} & \text{\color{red}a name?} \\
\hline
0 & \text{successor} & 1 + b & 1 + b & \text{predecessor} & b - 1 \\
1 & \text{addition} & a + b & 2 + b & \text{subtraction} & b - a \\
2 & \text{multiplication} & a \cdot b & 2b + 3 & \text{division} & \frac{b}{a}\\
3 & \text{exponentiation} & a^b & 2^{b + 3} - 3 & \text{logarithm} & \mathsf{log}_a b \\
[2pt]
4 & \text{tetration} & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \begin{array}{@{}c@{}}\text{iterated}\\[-3pt]\text{logarithm}\end{array} & \mathsf{log}^*_a b
\end{array}
\end{centermath}
\caption{Hyperoperations, Ackermann functions and inverses.}
\label{table: hyperop-ack-inv}
\end{table}

The kludge has three parts. First, the Ackermann hierarchy is related
to the hyperoperation hierarchy with $a$ set to $2$, ie $2[n]b$;
second, for $n>0$, $\Ack_n$ repeats the previous hyperoperation $2[n-1]b$
%{\color{magenta}(\textbf{not} the previous $\Ack_{n-1}$!)} 
three extra times;
lastly, $\Ack_n$ subtracts three\footnote{$\Ack_{1}$ and $\Ack_{2}$ do not break this pattern: $2 + (b + 3) - 3 = 2 + b$, and $2 \cdot (b + 3) - 3 = 2b + 3$.}.
It is worth studying and inverting the hyperoperations before handling the Ackermann function.

%Our initial goal of inverting the Ackermann function can thus be broken
%into three parts: first, inverting each individual member of the hyperoperation hierarchy; second, using these individual inverses to invert the diagonal hyperoperation {\color{red}$a [n] n$}; and lastly adjusting for the kludge.

%The kludge seems to be a conjugacy between the Ackermann hierarchy and the hyperoperations at $a=2$. For each $n$, we note $\Ack_n = \phi^{-1}\circ \left(2[n]\right)\circ\phi$ where $\phi(n) = n+3$.\footnote{It might appear that $\Ack_{1}$ and $\Ack_{2}$ break this pattern, but they do not since $2 + (b + 3) - 3 = 2 + b$ and $2 \cdot (b + 3) - 3 = 2b + 3$.}


%\[
%\begin{array}{c|cccc|c}
%n & \quad a [n] b \quad  & \quad a \uparrow^{n-2} b \quad & \quad \Ack(n,b) \quad & \text{function} & \text{inverse} \\
%\hline
%0 & 1 + b & {-} & 1 + b & \text{successor} & \text{predecessor} \\
%1 & a + b & {-} & 2 + b & \text{addition} & \text{subtraction} \\
%2 & a \cdot b & a \cdot b & 2b + 3 & \text{multiplication} & \text{division} \\
%3 & a^b & a^b & 2^{b + 3} - 3 & \text{exponentiation} & \text{logarithm} \\
%[2pt]
%4 & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{a^{.^{.^{.^a}}}}_b & \underbrace{2^{.^{.^{.^2}}}}_{b+3} - 3 & \text{tetration} & \begin{array}{@{}c@{}}\text{iterated}\\[-2pt]\text{logarithm}\end{array}
%\end{array}
%\]

\subsection{Increasing functions and their inverses}
Defining increasing functions is often significantly simpler than defining their inverses.
The Church numeral encodings of addition, multiplication, and even exponentiation
are each simpler than their corresponding inverses, \emph{i.e.} subtraction, division, and logarithm. Similarly, defining multiplication in Gallina~\cite{coq} is trivial, but defining division is unexpectedly painful.\\[5pt]

\begin{minipage}[c]{0.4\textwidth}
\begin{tabular}{@{}l@{\qquad \qquad}@{\qquad \qquad}l}
\begin{lstlisting}
Fixpoint mult a b :=
 match a with
 | 0 => 0
 | S a' => b + mult a' b
 end.
\end{lstlisting}
&
\begin{lstlisting}
Fixpoint div a b :=
 match a with
 | 0 => 0
 | _ => 1 + div (a - b) b
 end.
\end{lstlisting}
\end{tabular} \\[5pt]
\end{minipage}

\noindent The definition of \li{mult} is of course accepted by Coq; indeed this
is precisely how multiplication is defined in the standard library.  The function
\li{div} should calculate multiplication's upper inverse,
\emph{i.e.} $\li{div}~x~y \mapsto^{*} \lceil \frac{x}{y} \rceil$, but the definition
is rejected by Coq's termination checker.  Coq worries that
\li{a - b} might not be structurally smaller than \li{a}, since
subtraction is ``just another function,'' and is thus treated opaquely. Indeed, Coq
is right to be nervous: \li{div} will not terminate
when $\li{a}>0$ and $\li{b}=0$.

Of course, division \emph{can} be defined, but an elegant definition is a little
subtle. We certainly need to do more than just check that $\li{b}>0$.
Two standard techniques are to define a custom termination measure~\cite{chlipala};
and to define a straightforward function augmented with an
extra \li{nat} parameter that denotes a ``gas'' value that decreases at each recursive
call~\cite{gasperson}.  Both techniques are vaguely unsatisfying and neither is ideal
for our purposes: the first can be hard to generalize and the second requires a method to calculate the appropriate amount of gas.  For instance, calculating the amount of gas to compute $\alpha(100)$ the
``canonical'' way, \emph{e.g.} at least $2^{2^{2^{65,536}}}$, is problematic for many reasons,
not least because we cannot use the inverse Ackerman function in its own termination argument.
%One method is to define a custom termination measure~\cite{Chlipala?}, but this is
%both vaguely unsatisfying and not always easy to generalize.
Realizing this, the standard library employs
a cleverer approach to define division, but we are not aware of any explanation 
of the technique used, and we also find it hard to extend that technique 
to other functions of the natural sequence of functions.  One indication
of this difficulty is that the Coq standard library does not include a $\mathsf{log}_b$ function\footnote{Coq's standard library \textbf{does} include a $\mathsf{log}_2$ function, but
change-of-base does not work on \li{nat}:
$\left \lfloor \frac{\lfloor \mathsf{log}_2 100 \rfloor}{\lfloor \mathsf{log}_2 7 \rfloor} \right \rfloor = 3 \; \not= \; 2 = \lfloor \mathsf{log}_7 100 \rfloor$.  
%{\color{blue} efficiency of standard library version?} {\color{red} SSReflect?}
}, to say nothing of a $\mathsf{log}^{*}_b$ or the inverse Ackermann.
%We discuss alternative approaches further in \cref{sec:related}.

\subsection{Contributions}
%\marginpar{Let's carefully double-check this section once the rest of the paper is close to final form.} 
We provide a complete solution to inverting each individual function in the hyperoperation/Ackermann hierarchy,
as well as the diagonal Ackermann function itself.  All our functions are structurally recursive, so
Coq is immediately convinced of their termination.  Moreover, all our functions run in linear
time.  Finally, our techniques are extremely succinct: the code to invert the diagonal Ackermann function fits in a single page of this paper, and our entire Coq development is about 1,000 lines. The rest of this paper is organized as follows.
%, so they are as asymptotically efficient as they can be, and are thus suitable for extraction\footnote{A criterion more useful in practice for $\mathsf{log}_b$ and perhaps $\mathsf{log}^*_b$ than the other members of the hierarchy.}
%\begin{itemize}
%\item[\S\ref{blah}] We explain our core techniques of \emph{repeaters} and \emph{countdowns}. {\color{red}that allow us to define each level of the Ackermann hierarchy---and their upper inverses---in a straightforward and uniform manner.} We show how countdowns, in particular, can be written
%    structurally recursively.
%\item[\S\ref{blah}] We show how to use our techniques to define the Ackermann and upper inverse Ackermann functions themselves. {\color{red}Add inverse hyperop?}
%\item[\S\ref{blah}] We detail a few optimizations that improve the running time of our individual hierarchy inverse functions from $O(n^2)$ to $O(n)$, and then further improve the running time of our inverse Ackermann function from $O\big(n \cdot \alpha(n)\big)$ to $O(n)$.
%\item[\S\ref{blah}] We extend our functions in several useful ways: to the two-argument inverse Ackermann, lower inverses, and binary representations.
%\item[\S\ref{blah}] We discuss related work and give some closing thoughts.
%\end{itemize}
\begin{itemize}
	\item[\S\ref{sec: countdown-repeater}] We show a formal definition for hyperoperations, explain how to encode hyperoperations and the Ackermann function in Coq, and discuss our concept of \emph{repeater}.
	\item[\S\ref{sec:countdown}] We define a notion of upper inverses and show how to invert repeater with \emph{countdown}.
	\item[\S\ref{sec: inv-hyperop}] We show how to use our techniques to define the inverse hyperoperations, whose notable members include division, logarithm and iterated logarithm with arbitrary base; and sketch a method to find the Inverse Ackermann function.
	\item[\S\ref{sec: inv-ack}] We detail a specialized computation for the Inverse Ackermann function, improving over the sketch's running time of $\bigO\big(n^2\big)$ to $\bigO(n)$.
	\item[\S\ref{sec: discussion}] We discuss the two-argument inverse Ackermann function and survey related work.
\end{itemize}
All our techniques are mechanized in Coq and our code is available 
online~\cite{inv-ack}. Further, definitions presented here 
are \href{https://github.com/inv-ack/inv-ack}{hyperlinked} to appropriate points in our codebase to allow readers to explore correctness proofs and associated lemmas that were elided in this paper.
%, and
%each theorem and definition is hyperlinked (in {\color{cyan}cyan}) to our Coq development for %browsing.

